# -*- coding: utf-8 -*-
"""Diamond Price Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o4pFl-rmsENnvjWjCnS4CzfHl9g75bq8

#Diamond Price Prediction

### Context

Diamond is one of the precious stones which are always in huge demand in the investment market. Diamonds are also used in many industrial applications like cutting but it is mostly used as a gemstone. The actual price of a diamond however is determined by a gemologist after examining its various features such as its carat, cut, color, and clarity. Dimensions of a diamond is also a very important parameter to determine its worth. Nearly, 142 million carats of diamonds were produced worldwide in 2019 alone. This makes it very important to come up with some smart technique to estimate its worth.

---

### Problem Statement

A diamond distributor decided to put almost 2000 diamonds for auction. A jewellery company is interested in making a bid to purchase these diamonds in order to expand their business. As a data scientist, your job is to build a prediction model to predict the price of diamonds so that your company knows how much it should bid.

---

### Data Description

The **diamonds** dataset contains the prices and other attributes of almost 54,000 diamonds. Following are the attributes:  


|Column|Description|
|---:|:---|
|`carat`|weight of the diamond|
|`cut`|quality of the cut|
|`color`|diamond colour, from J (worst) to D (best)|
|`clarity`|a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))|
|`table`|The width of the diamond's table expressed as a percentage of its average diameter|
|`price`|price in US dollars|
|`x`|length in mm|
|`y`|width in mm|
|`z`|depth in mm|
|`depth`|total depth percentage = $\frac{2z}{x + y}$|

  **Dataset Link:** https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/diamonds.csv

---

### Things To Do

1. Explore the diamond dataset by creating the following plots:
   - Box plots between each categorical feature and the `price`.
   - Scatter plots between the numerical features and the `price`.
   
2. Convert categorical attributes into numerical attributes.

3. Create a correlation heatmap for all the columns.

4. Build a linear regression model by selecting the most relevant features to predict the price of diamonds.

5. Reduce multicollinearity (if exists) by eliminating highly correlated and high VIF features.

5. Evaluate the linear regression model by calculating the parameters such as coefficient of determination, MAE, MSE, RMSE, mean of residuals and by checking for homoscedasticity.

---

#### 1. Import Modules and Load Dataset

Link to the dataset: https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/diamonds.csv
"""

# Import the required modules and load the dataset.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/diamonds.csv')
df.head()

# Get the information on DataFrame.
df.info()
#df['price'].mean()

# Check if there are any null values. If any column has null values, treat them accordingly
df.isna().sum()

# Drop 'Unnamed: 0' column as it is of no use
df.drop('Unnamed: 0',axis=1,inplace=True)
df.head()

"""---

#### 2. Exploratory Data Analysis

We need to predict the value of `price` variable, using other variables. Thus, `price` is the target or dependent variable and other columns except `price` are the features or the independent variables. 

Perform the following tasks:

- Create Box plots between each **categorical** variable and the target variable `price` to sense the distribution of values.

- Create the Scatter plots between each **numerical** variable and the target variable `price`. Determine which variable(s) shows linear relationship with the target variable `price`. 

- Create a normal distribution curve for the `price`.
"""

# Boxplot for 'cut' vs 'price'
plt.figure(figsize=[15,5])
sns.boxplot(x='cut',y='price',data=df)
plt.show()

# Boxplot for 'color' vs 'price'
plt.figure(figsize=[15,5])
sns.boxplot(x='color',y='price',data=df, palette='rainbow')
plt.show()

# Boxplot for 'clarity' vs 'price'

plt.figure(figsize=[15,5])
sns.boxplot(x='clarity',y='price',data=df)
plt.show()

# Create scatter plot with 'carat' on X-axis and 'price' on Y-axis
plt.figure(figsize=(15,4))
plt.scatter(df['carat'],df['price'])
plt.title('Scatter plot with carat on X-axis and price on Y-axis')
plt.xlabel('Carat')
plt.ylabel('Price')
plt.show()

# Create scatter plot with 'depth' on X-axis and 'price' on Y-axis

plt.figure(figsize=(15,4))
plt.scatter(df['depth'],df['price'])
plt.title('Scatter plot with depth on X-axis and price on Y-axis')
plt.xlabel('depth')
plt.ylabel('Price')
plt.show()

# Create scatter plot with 'table' on X-axis and 'price' on Y-axis
plt.figure(figsize=(15,4))
plt.scatter(df['table'],df['price'])
plt.title('Scatter plot with table on X-axis and price on Y-axis')
plt.xlabel('table')
plt.ylabel('Price')
plt.show()

# Create scatter plot with attribute 'x' on X-axis and 'price' on Y-axis

plt.figure(figsize=(15,4))
plt.scatter(df['x'],df['price'])
plt.title('Scatter plot with x on X-axis and price on Y-axis')
plt.xlabel('X')
plt.ylabel('Price')
plt.show()

# Create scatter plot with attribute 'y' on X-axis and 'price' on Y-axis

plt.figure(figsize=(15,4))
plt.scatter(df['y'],df['price'])
plt.title('Scatter plot with y on X-axis and price on Y-axis')
plt.xlabel('y')
plt.ylabel('Price')
plt.show()

# Create scatter plot with 'z' on X-axis and 'price' on Y-axis

plt.figure(figsize=(15,4))
plt.scatter(df['z'],df['price'])
plt.title('Scatter plot with z on X-axis and price on Y-axis')
plt.xlabel('z')
plt.ylabel('Price')
plt.show()

"""**Q:** Which attribute exhibit the best linear relationship with the target variable `price`?

**A:** attribute 'carat'.
"""

# Create a normal distribution curve for the `price`.


# Create a probablity density function for plotting the normal distribution
def prob_density_func(series):
  CONST = 1 / (series.std() * np.sqrt(2 * np.pi))
  power_of_e = - (series - series.mean()) ** 2 / (2 * series.var()) # 'pd.Series.var()' function returns the variance of the series.
  new_array = CONST * np.exp(power_of_e)
  return new_array


# Plot the normal distribution curve using plt.scatter()
print(df['price'].mean())
plt.figure(figsize=[15,5])
plt.scatter(df['price'],prob_density_func(df['price']))
plt.title('Normal Distribution Curve for Price')
plt.legend()
plt.show()

"""**Q:** What is the mean `price` of diamonds ?

**A:** 3932.8

---

#### 3. Feature Engineering

The dataset contains certain features that are categorical.  To convert these features into numerical ones, use `replace()` function of the DataFrame. 

**For example:** 

`df["column1"].replace({"a": 1, "b": 0}, inplace=True)` $\Rightarrow$ replaces all the `'a'` values with `1` and `'b'` values with `0` for feature `column1`. Use `inplace` boolean argument to to make changes in the DataFrame permanently.

Replace following values for `cut` column:

 - `Fair` with `1`
 - `Good` with `2`
 - `Very Good` with `3` 
 - `Premium` with `4`
 - `Ideal` with `5`

Replace following values for the `color` column:

- `D` with `1`
- `E` with `2`
- `F` with `3`
- `G` with `4`
- `H` with `5`
- `I` with `6`

Replace following values for the `clarity` column:

- `I1` with `1` 
- `SI2` with `2`
- `SI1` with `3`
- `VS2` with `4`
- `VS1` with `5`
- `VVS2` with `6`
- `VVS1` with `7`
- `IF` with `8`
"""

# Replace values of 'cut' column
df["cut"].replace({'Fair': 1,'Good': 2, 'Very Good': 3, 'Premium':  4, 'Ideal' : 5}, inplace=True)
df.head()

# Replace values of 'color' column
df["color"].replace({'D': 1,'E':2,'F' :3,'G':4,'H':5,'I':6,'J':7}, inplace=True)

df.head()

# Replace values of 'clarity' column

df["clarity"].replace({'I1':  1,'SI2' : 2,'SI1' : 3,'VS2':  4,'VS1':  5,'VVS2':  6,'VVS1' : 7,'IF': 8}, inplace=True)

#df.info()
df.head()
#df['color'].value_counts()

"""---

#### 4. Model Training 

Build a multiple linear regression model  using all the features of the dataset. Also, evaluate the model by calculating $R^2$, MSE, RMSE, and MAE values.
"""

# Create a list of feature variables.
features=list(df.columns.values)
features.remove('price')
features

# Build multiple linear regression model using all the features
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression


X=df[features]
y=df['price']


# Split the DataFrame into the train and test sets such that test set has 33% of the values.

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)

# Build linear regression model using the 'sklearn.linear_model' module.
sklearn_lin_reg = LinearRegression()
sklearn_lin_reg.fit(X_train, y_train)


# Print the value of the intercept

print("\nConstant".ljust(15, " "), f"{sklearn_lin_reg.intercept_:.6f}")

# Print the names of the features along with the values of their corresponding coefficients.

for item in list(zip(X.columns.values, sklearn_lin_reg.coef_)):
  print(f"{item[0]}".ljust(15, " "), f"{item[1]:.6f}")

# Evaluate the linear regression model using the 'r2_score', 'mean_squared_error' & 'mean_absolute_error' functions of the 'sklearn' module.

from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error
y_test_pred = sklearn_lin_reg.predict(X_test)
y_train_pred = sklearn_lin_reg.predict(X_train)

print(f"\n\nTrain Set\n{'-' * 50}")
print(f"R-squared: {r2_score(y_train, y_train_pred):.6f}")
print(f"Mean Squared Error: {mean_squared_error(y_train, y_train_pred):.3f}")
print(f"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.3f}")
print(f"Mean Absolute Error: {mean_absolute_error(y_train, y_train_pred):.3f}")


print(f"\n\nTest Set\n{'-' * 50}")
print(f"R-squared: {r2_score(y_test, y_test_pred):.3f}")
print(f"Mean Squared Error: {mean_squared_error(y_test, y_test_pred):.3f}")
print(f"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_test_pred)):.3f}")
print(f"Mean Absolute Error: {mean_absolute_error(y_test, y_test_pred):.3f}")

"""**Q:** What is the $R^2$ (R-squared) value for this model?

**A:** 0.907

---

### 5. Dealing with Multicollinearity

Create a heatmap among all variables to identify a set of features which are highly correlated with each other.
"""

# Heatmap to pinpoint the columns in the 'df' DataFrame exhibiting high correlation

plt.figure(figsize=[16,6])
sns.heatmap(df.corr(),annot=True)
plt.show()

"""**Q:** Which features are highly correlated with `price`?

**A:** x,y,z and carat

**Q:** Is there multicollinearity in the dataset?

**A:** yes

Let's consider the feature `carat` as it is highly correlated with the target variable `price`. Perform the following tasks:
1. Drop the features which are highly correlated with `carat`.
2. Calculate VIF (Variance Inflation Factor) for the remaining features.
"""

# Drop features highly correlated with 'carat'
#print(features)
features.remove('y')
features.remove('x')
features.remove('z')
features

# Again build a linear regression model using the remaining features

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression


X=df[features]
y=df['price']


# Split the DataFrame into the train and test sets such that test set has 33% of the values.

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)

# Build linear regression model using the 'sklearn.linear_model' module.
sklearn_lin_reg = LinearRegression()
sklearn_lin_reg.fit(X_train, y_train)


# Print the value of the intercept

print("\nConstant".ljust(15, " "), f"{sklearn_lin_reg.intercept_:.6f}")

# Print the names of the features along with the values of their corresponding coefficients.

for item in list(zip(X.columns.values, sklearn_lin_reg.coef_)):
  print(f"{item[0]}".ljust(15, " "), f"{item[1]:.6f}")

# Evaluate the linear regression model using the 'r2_score', 'mean_squared_error' & 'mean_absolute_error' functions of the 'sklearn' module.
from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error
y_test_pred = sklearn_lin_reg.predict(X_test)
y_train_pred = sklearn_lin_reg.predict(X_train)

print(f"\n\nTrain Set\n{'-' * 50}")
print(f"R-squared: {r2_score(y_train, y_train_pred):.6f}")
print(f"Mean Squared Error: {mean_squared_error(y_train, y_train_pred):.3f}")
print(f"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.3f}")
print(f"Mean Absolute Error: {mean_absolute_error(y_train, y_train_pred):.3f}")


print(f"\n\nTest Set\n{'-' * 50}")
print(f"R-squared: {r2_score(y_test, y_test_pred):.3f}")
print(f"Mean Squared Error: {mean_squared_error(y_test, y_test_pred):.3f}")
print(f"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_test_pred)):.3f}")
print(f"Mean Absolute Error: {mean_absolute_error(y_test, y_test_pred):.3f}")

"""Now eliminate the features having VIF values above 10 (if any)."""

# Calculate the VIF values for the remaining features using the 'variance_inflation_factor' function.
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm

X_trainsm=sm.add_constant(X_train)
# Add a constant to feature variables 
vif=pd.DataFrame()
vif['Features']=X_trainsm.columns
vif['VIF']=[ variance_inflation_factor(X_trainsm.values,i) for i in range(X_trainsm.values.shape[1])]
vif['VIF']=round(vif['VIF'],2)
vif=vif.sort_values(by='VIF',ascending=False)
vif

# Create a dataframe that will contain the names of the feature variables and their respective VIFs

"""**Q**: Which of the features have VIF values above 10?

**A**: All features have values less than 10.

Proceed with the below 4 code cells only if any of the features have VIF value above 10, otherwise jump to **6. Residual (Error) Analysis** section.
"""









"""---

#### 6. Residual (Error) Analysis

Perform residual analysis to check if the residuals (errors) are normally distributed or not (which is one of the assumption of linear regression). For this, plot the  histogram of the residuals.
"""

y_train_pred = sklearn_lin_reg.predict(X_train)
# Create a histogram for the errors obtained in the predicted values for the train set.


errors_train=y_train-y_train_pred

plt.figure(figsize=[15,5])
plt.hist(errors_train,bins='sturges',edgecolor='green')
plt.title('Histogram for the errors obtained in the predicted values for the train set')
plt.axvline(x=errors_train.mean(),label=f'Mean of error ={errors_train.mean():.3f}',color='red')
plt.xlabel('Train_set errors ')
plt.legend()
plt.show()

y_test_pred = sklearn_lin_reg.predict(X_test)
# Create a histogram for the errors obtained in the predicted values for the test set.


errors_test=y_test-y_test_pred

plt.figure(figsize=[15,5])
plt.hist(errors_test,bins='sturges',edgecolor='green')
plt.title('Histogram for the errors obtained in the predicted values for the test set')
plt.axvline(x=errors_test.mean(),label=f'Mean of error ={errors_test.mean():.3f}',color='red')
plt.xlabel('Test_set errors ')
plt.legend()
plt.show()

"""**Q:** Is the mean of errors equal to 0 for train set?

**A:** Yes

**Q:** Is the mean of errors equal to 0 for test set?

**A:** No, -10.131

---

#### 7. Verify Homoscedasticity 

Check for Homoscedasticity (constant variance) by creating a scatter plot between the errors and the target variable. Determine whether there is some kind of relationship between the error and the target variable.
"""

# Create a scatter plot between the errors and the dependent variable for the train set.


plt.figure(figsize=[15,5])
plt.scatter(y_train,errors_train)
plt.axhline(y=errors_train.mean(),label=f'Mean of errors={errors_train.mean():.3f}',color='red')
plt.legend()
plt.show()

"""**Q:** Do you find any pattern or trend in the scatter plot? Whether the residuals exhibit constant variance around mean of errors?

**A:** There is No Pattern . Yes there is presence of homoscedasticity.


"""